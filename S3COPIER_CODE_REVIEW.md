# S3Copier.java Code Review

**File**: `app/src/main/java/com/procure/thg/cockroachdb/S3Copier.java`
**Review Date**: 2025-11-18
**Status**: Review Complete - Issues Identified

---

## Executive Summary

The S3Copier class has solid overall structure with good pagination and error handling strategies. However, there are critical bugs (null pointer risk), significant code duplication, and inconsistencies that need addressing. Priority should be given to the critical and high-priority items.

**Total Issues Found**: 10
**Critical**: 1 | **High**: 3 | **Medium**: 4 | **Minor**: 2

---

## Critical Issues

### 1. NullPointerException Risk in `syncObjectMetadata()` - Line 290

**Severity**: üî¥ CRITICAL
**Location**: `syncObjectMetadata()` method, line 290

**Problem**:
```java
metadata.put("x-amz-meta-last-modified", sourceHeadResponse.lastModified().toString());
```

The `lastModified()` method can return null, and calling `.toString()` on null will throw a NullPointerException. This will crash the metadata sync operation.

**Evidence**:
- Compare to `copyObject()` method (lines 160-162) which **properly checks for null**:
  ```java
  if (objectStream.response().lastModified() != null) {
      metadata.put("x-amz-meta-last-modified", objectStream.response().lastModified().toString());
  }
  ```

**Root Cause**: Inconsistent null-handling pattern between two methods that perform similar operations

**Fix Required**:
```java
// Replace line 290 with:
if (sourceHeadResponse.lastModified() != null) {
    metadata.put("x-amz-meta-last-modified", sourceHeadResponse.lastModified().toString());
}
```

**Impact if Not Fixed**: Sync operations will crash with NullPointerException when source objects have missing lastModified timestamps

**Effort**: 5 minutes

---

## High-Priority Issues

### 2. Code Duplication: Pagination Logic - Lines 56-101 vs 198-243

**Severity**: üü† HIGH
**Location**: Two methods duplicate pagination logic:
- `copyRecentObjects()` lines 56-101
- `syncMetaDataRecentObjects()` lines 198-243

**Problem**:
Both methods implement nearly identical pagination logic to list and iterate through S3 objects. This is approximately 90+ lines of duplicated code that:
- Violates DRY (Don't Repeat Yourself) principle
- Creates maintenance burden - bug fixes must be made in two places
- Increases risk of divergence between implementations
- Makes code harder to test

**Duplicated Code Sections**:
1. **Pagination Setup** (lines 56-62 vs 198-204):
   ```java
   ListObjectsV2Request.Builder requestBuilder = ListObjectsV2Request.builder()
       .bucket(sourceBucket)
       .encodingType(EncodingType.URL);
   if (!sourceFolder.isEmpty()) {
       requestBuilder.prefix(sourceFolder);
   }
   ListObjectsV2Request listObjectsV2Request = requestBuilder.build();
   ```

2. **Initial List Request** (lines 65-70 vs 207-212):
   ```java
   ListObjectsV2Response listObjectsV2Response;
   try {
       listObjectsV2Response = sourceClient.listObjectsV2(listObjectsV2Request);
   } catch (Exception e) {
       LOGGER.log(SEVERE, String.format("Failed to list objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);
       return;
   }
   ```

3. **Pagination Loop** (lines 72-101 vs 214-243):
   ```java
   do {
       for (S3Object s3Object : listObjectsV2Response.contents()) {
           final String key = s3Object.key();
           if (s3Object.lastModified().isAfter(threshold)) {
               try {
                   // action here (copyObject or syncObjectMetadata)
               } catch (Exception e) { ... }
           }
       }
       // Handle pagination continuation...
   } while (Boolean.TRUE.equals(listObjectsV2Response.isTruncated()));
   ```

**Fix Required**:
Extract pagination logic into a reusable helper method that accepts a callback/consumer for the action to perform on each object:

```java
private void iterateRecentObjects(final Instant threshold, Consumer<String> objectAction) {
    LOGGER.log(INFO, "Starting to iterate recent objects from {0}/{1}",
            new Object[]{sourceBucket, sourceFolder});

    ListObjectsV2Request.Builder requestBuilder = ListObjectsV2Request.builder()
            .bucket(sourceBucket)
            .encodingType(EncodingType.URL);
    if (!sourceFolder.isEmpty()) {
        requestBuilder.prefix(sourceFolder);
    }
    ListObjectsV2Request listObjectsV2Request = requestBuilder.build();

    ListObjectsV2Response listObjectsV2Response;
    try {
        listObjectsV2Response = sourceClient.listObjectsV2(listObjectsV2Request);
    } catch (Exception e) {
        LOGGER.log(SEVERE, String.format("Failed to list objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);
        return;
    }

    do {
        for (S3Object s3Object : listObjectsV2Response.contents()) {
            final String key = s3Object.key();
            if (s3Object.lastModified().isAfter(threshold)) {
                try {
                    objectAction.accept(key);
                } catch (Exception e) {
                    LOGGER.log(SEVERE, String.format("Failed to process object %s: %s", key, e.getMessage()), e);
                }
            }
        }

        if (Boolean.TRUE.equals(listObjectsV2Response.isTruncated())) {
            requestBuilder = ListObjectsV2Request.builder()
                    .bucket(sourceBucket)
                    .encodingType(EncodingType.URL)
                    .continuationToken(listObjectsV2Response.nextContinuationToken());
            if (!sourceFolder.isEmpty()) {
                requestBuilder.prefix(sourceFolder);
            }
            listObjectsV2Request = requestBuilder.build();

            try {
                listObjectsV2Response = sourceClient.listObjectsV2(listObjectsV2Request);
            } catch (Exception e) {
                LOGGER.log(SEVERE, String.format("Failed to list next page of objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);
                break;
            }
        }
    } while (Boolean.TRUE.equals(listObjectsV2Response.isTruncated()));
    LOGGER.log(INFO, "Finished iterating objects.");
}
```

Then refactor both methods to use it:
```java
public void copyRecentObjects(final long thresholdSeconds) {
    final Instant threshold = Instant.now().minus(thresholdSeconds, ChronoUnit.SECONDS);
    iterateRecentObjects(threshold, this::copyObject);
}

public void syncMetaDataRecentObjects(final long thresholdSeconds) {
    final Instant threshold = Instant.now().minus(thresholdSeconds, ChronoUnit.SECONDS);
    iterateRecentObjects(threshold, this::syncObjectMetadata);
}
```

**Impact if Not Fixed**:
- Pagination bugs will need to be fixed in two places
- Code maintenance becomes error-prone
- Harder to understand intent of the class

**Effort**: 30 minutes (including testing)

---

### 3. Inconsistent Exception Handling Behavior

**Severity**: üü† HIGH
**Location**: Multiple methods have different exception handling patterns

**Problem**:
The methods handle exceptions inconsistently, making it unclear whether operations have failed:

| Method | Behavior | Line(s) |
|--------|----------|---------|
| `copyRecentObjects()` | Catches exception, logs SEVERE, **returns** silently | 67-69 |
| `copyObject()` | Catches exception, logs SEVERE, **throws** exception | 186-189 |
| `syncMetaDataRecentObjects()` | Catches exception, logs SEVERE, **breaks** loop | 238-240 |
| `syncObjectMetadata()` | Catches exception, logs SEVERE, **throws** exception | 306-309 |

**Evidence**:

In `copyRecentObjects()` (line 67-69):
```java
} catch (Exception e) {
    LOGGER.log(SEVERE, String.format("Failed to list objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);
    return;  // ‚Üê Silent failure - caller doesn't know operation failed
}
```

In `copyObject()` (line 186-189):
```java
} catch (Exception e) {
    LOGGER.log(SEVERE, String.format("Failed to copy object from %s/%s to %s/%s: %s",
            sourceBucket, sourceKey, targetBucket, targetKey, e.getMessage()), e);
    throw e;  // ‚Üê Exception propagates
}
```

**Root Cause**: No defined contract for error handling - different error scenarios handled differently

**Impact**:
- Callers cannot reliably detect failures
- Some exceptions are silently swallowed
- Monitoring/alerting systems can't distinguish between success and partial failure

**Fix Required**:
Define explicit error handling contract. Options:

**Option A: Fail-Fast (Recommended for production safety)**
```java
public void copyRecentObjects(final long thresholdSeconds) throws Exception {
    // ... instead of returning on exception, throw it
}

public void syncMetaDataRecentObjects(final long thresholdSeconds) throws Exception {
    // ... instead of breaking on exception, throw it
}
```

**Option B: Accumulate Errors**
Track failures and return a report:
```java
public class CopyResult {
    public int successCount;
    public int failureCount;
    public List<String> failedKeys;
}

public CopyResult copyRecentObjects(final long thresholdSeconds) {
    // ... accumulate failures, return result
}
```

**Implementation Steps**:
1. Decide on error handling strategy (fail-fast vs accumulate)
2. Update method signatures
3. Update documentation
4. Update tests to verify exception handling
5. Update caller code (App.java) to handle exceptions

**Effort**: 20 minutes (implementation) + 15 minutes (testing)

---

### 4. Incorrect Error Message in `syncMetaDataRecentObjects()`

**Severity**: üü† HIGH
**Location**: Line 221 in `syncMetaDataRecentObjects()` method

**Problem**:
```java
LOGGER.log(SEVERE, String.format("Failed to copy object %s: %s", key, e.getMessage()), e);
```

This says "Failed to **copy**" but the method is for **syncing metadata**, not copying objects. This misleading message will confuse developers during debugging.

**Evidence**:
- Method name: `syncMetaDataRecentObjects()` - clearly about syncing
- Operation: Calls `syncObjectMetadata()` - not copy
- Message: Says "copy" - inconsistent

**Correct Message**:
```java
LOGGER.log(SEVERE, String.format("Failed to sync metadata for object %s: %s", key, e.getMessage()), e);
```

**Impact if Not Fixed**:
- Developers debugging logs will be confused about what actually failed
- Harder to diagnose issues in production
- Potential misreporting of issues to upstream systems

**Effort**: 2 minutes

---

## Medium-Priority Issues

### 5. Inconsistent Logging Style

**Severity**: üü° MEDIUM
**Location**: Throughout the class

**Problem**:
The class mixes two logging styles inconsistently:

**Style 1: Parameterized Logging** (Preferred - lines 52-53, 102, etc.)
```java
LOGGER.log(INFO, "Starting to copy objects from {0}/{1} to {2}/{3}",
        new Object[]{sourceBucket, sourceFolder, targetBucket, targetFolder});
```

**Style 2: String.format** (Anti-pattern - lines 68, 79, 97, etc.)
```java
LOGGER.log(SEVERE, String.format("Failed to list objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);
```

**Problem with String.format style**:
- String concatenation happens **even if the log level is disabled**
- Wastes CPU cycles and memory
- Parameterized logging is lazy - formatting only happens if log level is enabled

**Impact**:
- Performance degradation in production (where log levels are typically higher)
- Inconsistent code style hurts readability

**Locations with String.format**:
- Line 68, 79, 97 (in `copyRecentObjects()`)
- Line 143 (in `copyObject()`)
- Line 187-188 (in `copyObject()`)
- Line 210, 221, 239 (in `syncMetaDataRecentObjects()`)
- Line 283-284, 307-308 (in `syncObjectMetadata()`)

**Fix Required**:
Replace all `String.format()` calls with parameterized logging:

```java
// Before (anti-pattern):
LOGGER.log(SEVERE, String.format("Failed to list objects in %s/%s: %s", sourceBucket, sourceFolder, e.getMessage()), e);

// After (correct):
LOGGER.log(SEVERE, "Failed to list objects in {0}/{1}: {2}",
        new Object[]{sourceBucket, sourceFolder, e.getMessage()});
```

**Effort**: 15 minutes (find and replace multiple locations)

---

### 6. Memory Risk with Large Files - Lines 177-182

**Severity**: üü° MEDIUM
**Location**: `copyObject()` method, lines 177-182

**Problem**:
```java
if (contentLength != null && contentLength >= 0) {
    targetClient.putObject(putRequest, RequestBody.fromInputStream(objectStream, contentLength));
} else {
    byte[] content = objectStream.readAllBytes();  // ‚Üê DANGEROUS for large files
    targetClient.putObject(putRequest, RequestBody.fromBytes(content));
}
```

The fallback case reads the **entire file into memory**. For large objects (GBs), this can cause OutOfMemoryError and crash the application.

**Scenarios that trigger fallback**:
- If `contentLength` is null (some S3-compatible services don't report it)
- If `contentLength` is -1 or negative (indicates unknown size)

**Impact**:
- Application crashes with OutOfMemoryError for large objects
- Unpredictable behavior depending on object size and available heap

**Risk Assessment**:
- Low probability if using standard AWS S3 (always returns contentLength)
- Medium-High probability if using S3-compatible services (Ceph, MinIO may not return contentLength)

**Fix Options**:

**Option A: Enforce contentLength (Strict)**
```java
if (contentLength == null || contentLength < 0) {
    LOGGER.log(WARNING, "Cannot determine content length for {0}/{1}, skipping",
            new Object[]{sourceBucket, sourceKey});
    return;
}
targetClient.putObject(putRequest, RequestBody.fromInputStream(objectStream, contentLength));
```

**Option B: Set Memory Limit (Safe)**
```java
final long MAX_MEMORY_COPY = 1024 * 1024 * 100; // 100 MB limit

if (contentLength != null && contentLength >= 0 && contentLength < MAX_MEMORY_COPY) {
    targetClient.putObject(putRequest, RequestBody.fromInputStream(objectStream, contentLength));
} else if (contentLength != null && contentLength >= 0) {
    // Content length known and reasonable, use streaming
    targetClient.putObject(putRequest, RequestBody.fromInputStream(objectStream, contentLength));
} else {
    // Unknown size or too large - log warning and skip
    LOGGER.log(WARNING, "Cannot safely copy object {0}/{1} (unknown or too large content length)",
            new Object[]{sourceBucket, sourceKey});
    throw new IOException("Content length not available for streaming copy");
}
```

**Option C: Chunked Upload (Best)**
Use AWS S3 multipart upload API for large objects, but this requires more implementation.

**Recommendation**: Option B - set reasonable limit and fail explicitly rather than risking OutOfMemoryError

**Effort**: 20 minutes

---

### 7. Validation Gap: No Logging of Folder Normalization

**Severity**: üü° MEDIUM
**Location**: Constructor lines 28-38, `suffixFolderName()` method lines 40-49

**Problem**:
The `suffixFolderName()` method normalizes folder paths (adds trailing slashes), but:
- No visibility into what normalization was applied
- Difficult to debug path-related issues
- No warning if folder parameter was null or empty

**Current Code**:
```java
public S3Copier(final S3Client sourceClient, final String sourceBucket, final String sourceFolder,
                final S3Client targetClient, final String targetBucket, final String targetFolder,
                final boolean copyModified) {
    this.sourceClient = sourceClient;
    this.sourceBucket = sourceBucket;
    this.sourceFolder = suffixFolderName(sourceFolder);  // ‚Üê Applied, but no logging
    this.targetClient = targetClient;
    this.targetBucket = targetBucket;
    this.targetFolder = suffixFolderName(targetFolder);  // ‚Üê Applied, but no logging
    this.copyModified = copyModified;
}

private String suffixFolderName(final String folder) {
    if (folder != null && !folder.isEmpty()) {
        if (folder.endsWith("/")) {
            return folder;
        }
        return folder + "/";
    } else {
        return "";  // ‚Üê No indication if folder was null or empty
    }
}
```

**Issues**:
- If user passes `folder=""` vs `folder=null` vs `folder="path"` vs `folder="path/"`, all get different results but no visibility
- Makes debugging path issues harder
- No way to verify configuration at startup

**Fix Required**:
Add logging to make folder normalization visible:

```java
public S3Copier(final S3Client sourceClient, final String sourceBucket, final String sourceFolder,
                final S3Client targetClient, final String targetBucket, final String targetFolder,
                final boolean copyModified) {
    this.sourceClient = sourceClient;
    this.sourceBucket = sourceBucket;
    this.sourceFolder = suffixFolderName(sourceFolder);
    this.targetClient = targetClient;
    this.targetBucket = targetBucket;
    this.targetFolder = suffixFolderName(targetFolder);
    this.copyModified = copyModified;

    LOGGER.log(INFO, "S3Copier initialized: source={0}/{1}, target={2}/{3}, copyModified={4}",
            new Object[]{sourceBucket, this.sourceFolder, targetBucket, this.targetFolder, copyModified});
}

private String suffixFolderName(final String folder) {
    if (folder != null && !folder.isEmpty()) {
        if (folder.endsWith("/")) {
            return folder;
        }
        return folder + "/";
    } else {
        LOGGER.log(FINE, "Folder is null or empty, using bucket root");
        return "";
    }
}
```

**Effort**: 5 minutes

---

## Minor Issues

### 8. Potential Metadata Loss in `syncObjectMetadata()` - Line 289

**Severity**: üîµ MINOR
**Location**: `syncObjectMetadata()` method, line 289

**Problem**:
```java
Map<String, String> metadata = new HashMap<>(sourceHeadResponse.metadata());
```

The `HeadObject` API response may not include all custom metadata. According to AWS SDK documentation:
- `HeadObject` returns **limited metadata**
- `GetObject` returns **complete metadata**

If source object has extensive custom metadata, some might be lost.

**Evidence**:
- AWS S3 API limitation documented in SDK
- The `copyObject()` method (line 159) uses `GetObjectResponse.metadata()` which has more complete metadata
- Code comment at line 158: "// Use metadata from GetObjectResponse" - acknowledging the better source

**Current Flow**:
1. `syncObjectMetadata()` uses `HeadObject` ‚Üí gets limited metadata
2. `copyObject()` uses `GetObject` ‚Üí gets complete metadata

**Impact**:
- Custom metadata fields may not be synced
- Inconsistency between sync and copy operations

**Fix Options**:

**Option A: Use GetObject for Complete Metadata**
```java
// Instead of HeadObject for metadata retrieval:
GetObjectRequest getRequest = GetObjectRequest.builder()
        .bucket(sourceBucket)
        .key(sourceKey)
        .build();

Map<String, String> metadata;
try (ResponseInputStream<GetObjectResponse> objectStream = sourceClient.getObject(getRequest)) {
    metadata = new HashMap<>(objectStream.response().metadata());
} catch (Exception e) {
    // error handling
}
```

**Option B: Document Limitation**
```java
/**
 * Syncs metadata from source object to target object.
 * Note: Uses HeadObject which may not return all custom metadata fields.
 * For complete metadata sync, use copyRecentObjects() instead.
 */
public void syncObjectMetadata(final String sourceKey) { ... }
```

**Recommendation**: Document the limitation clearly in javadoc. Changing to GetObject requires downloading full object which defeats purpose of metadata-only sync.

**Effort**: 5 minutes (documentation) or 20 minutes (implementation change)

---

### 9. Magic String - Metadata Key

**Severity**: üîµ MINOR
**Location**: Lines 161 and 290

**Problem**:
The hardcoded string `"x-amz-meta-last-modified"` appears in multiple places:
- Line 161 in `copyObject()`
- Line 290 in `syncObjectMetadata()`

If this metadata key needs to change in the future, it must be updated in multiple places.

**Current Code**:
```java
// Line 161:
metadata.put("x-amz-meta-last-modified", objectStream.response().lastModified().toString());

// Line 290:
metadata.put("x-amz-meta-last-modified", sourceHeadResponse.lastModified().toString());
```

**Fix Required**:
Add class-level constant:

```java
public class S3Copier {
    private static final Logger LOGGER = Logger.getLogger(S3Copier.class.getName());
    private static final String METADATA_LAST_MODIFIED_KEY = "x-amz-meta-last-modified";

    // ... rest of class
}
```

Then replace all occurrences:
```java
// Line 161:
metadata.put(METADATA_LAST_MODIFIED_KEY, objectStream.response().lastModified().toString());

// Line 290:
metadata.put(METADATA_LAST_MODIFIED_KEY, sourceHeadResponse.lastModified().toString());
```

**Effort**: 3 minutes

---

### 10. Silent Skip on Key Mismatch in `copyObject()`

**Severity**: üîµ MINOR
**Location**: `copyObject()` method, lines 106-110

**Problem**:
```java
if (!sourceKey.startsWith(sourceFolder)) {
    LOGGER.log(WARNING, "Object key {0} does not start with expected prefix {1}, skipping",
            new Object[]{sourceKey, sourceFolder});
    return;
}
```

Silently skips objects that don't match the folder prefix. This could hide configuration errors:
- Wrong folder name configured
- S3 listing returned unexpected keys
- Folder prefix logic issue

**Impact**:
- Silent skip makes bugs harder to detect
- Could result in missed objects without obvious error
- In production, might hide systematic issues

**Better Approach**:
```java
if (!sourceKey.startsWith(sourceFolder)) {
    String errorMsg = String.format("Object key %s does not start with expected prefix %s", sourceKey, sourceFolder);
    LOGGER.log(SEVERE, errorMsg);
    throw new IllegalArgumentException(errorMsg);
}
```

Or if this is intentional (filtering behavior), document it clearly:
```java
if (!sourceKey.startsWith(sourceFolder)) {
    LOGGER.log(FINE, "Object key {0} is outside folder prefix {1}, skipping",
            new Object[]{sourceKey, sourceFolder});
    return;
}
```

**Recommendation**: This depends on intended behavior. If filtering is intentional, change log level from WARNING to FINE. If it's an error condition, throw exception.

**Effort**: 5 minutes

---

## Positive Aspects

‚úÖ **Good try-with-resources usage** (line 155) for automatic stream closing
‚úÖ **Proper pagination** with continuation tokens for large result sets
‚úÖ **Thoughtful ETag/size comparison** (lines 133-139) for detecting modifications
‚úÖ **Granular error handling** prevents partial failure from stopping entire batch
‚úÖ **Clear separation** between copy and metadata sync operations
‚úÖ **Handles both empty and populated folders** correctly

---

## Summary Table

| # | Issue | Severity | Effort | Category |
|---|-------|----------|--------|----------|
| 1 | NullPointerException on lastModified() | üî¥ Critical | 5 min | Bug |
| 2 | Pagination code duplication | üü† High | 30 min | Refactor |
| 3 | Inconsistent exception handling | üü† High | 20 min | Bug |
| 4 | Wrong error message "copy" vs "sync" | üü† High | 2 min | Bug |
| 5 | Inconsistent logging style (String.format) | üü° Medium | 15 min | Code Quality |
| 6 | Memory risk with large files | üü° Medium | 20 min | Bug |
| 7 | No logging of folder normalization | üü° Medium | 5 min | Observability |
| 8 | Potential metadata loss (HeadObject vs GetObject) | üîµ Minor | 5-20 min | Enhancement |
| 9 | Magic string for metadata key | üîµ Minor | 3 min | Code Quality |
| 10 | Silent skip on key mismatch | üîµ Minor | 5 min | Design |

---

## Implementation Roadmap

### Phase 1: Critical Fixes (5 minutes)
- [ ] Fix NullPointerException on lastModified() (Issue #1)

### Phase 2: High-Priority Fixes (52 minutes)
- [ ] Fix incorrect error message (Issue #4) - 2 min
- [ ] Implement consistent exception handling (Issue #3) - 20 min
- [ ] Extract pagination logic (Issue #2) - 30 min

### Phase 3: Medium-Priority Improvements (45 minutes)
- [ ] Replace String.format with parameterized logging (Issue #5) - 15 min
- [ ] Add memory limit for large file copying (Issue #6) - 20 min
- [ ] Add folder normalization logging (Issue #7) - 5 min
- [ ] Document metadata completeness limitation (Issue #8) - 5 min

### Phase 4: Code Quality (8 minutes)
- [ ] Extract metadata key to constant (Issue #9) - 3 min
- [ ] Clarify key mismatch handling (Issue #10) - 5 min

**Total Estimated Time**: ~110 minutes

---

## Testing Considerations

When implementing fixes, ensure:
1. Create tests for null lastModified() scenario
2. Add tests verifying pagination continuation works
3. Test exception propagation in all paths
4. Verify metadata sync completes without NPE
5. Test with large file (>100MB) to validate memory usage
6. Verify error messages are accurate
7. Test with S3-compatible services (Ceph, MinIO) for contentLength edge cases

---

## Notes for Future Implementation

- **Branch Strategy**: Create feature branches for each phase or issue group
- **Testing**: Add unit tests for pagination helper method
- **Documentation**: Update class javadoc with exception behavior after Issue #3 fix
- **Backward Compatibility**: Changes are internal; no API changes needed
- **Performance**: Pagination extraction will slightly improve performance (no overhead)